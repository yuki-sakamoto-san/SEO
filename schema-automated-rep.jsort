# ============================================
# STEP 1: Install dependencies
# ============================================
!pip -q install extruct w3lib[html] lxml tldextract httpx[http2] selectolax[parser] pandas tqdm aiohttp aiolimiter urllib3==2.2.2
# Optional, if you need JS-rendered schema:
# !pip -q install playwright
# !playwright install chromium

# ============================================
# STEP 2: Mount Google Drive
# ============================================
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os, datetime

# Timestamped output folder in Drive
ts = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
DRIVE_OUT = f"/content/drive/MyDrive/schema_audit/{ts}"
os.makedirs(DRIVE_OUT, exist_ok=True)

print("Results will be saved to:", DRIVE_OUT)

# ============================================
# STEP 3: Config
# ============================================
DOMAIN = input("Enter the full domain URL (e.g., https://www.example.com): ").strip()

MAX_PAGES         = 500                         # crawl budget
PRIORITY          = "both"                      # "sitemap" | "crawl" | "both"
RESPECT_ROBOTS    = True
REQUESTS_PER_SEC  = 3
TIMEOUT_SECS      = 20
RENDER_JS         = False                       # True if schema is injected via JS
INCLUDE_QUERYSTR  = False
ALLOWED_PATHS     = []                          # e.g., ["/ja_JP/"]
EXCLUDE_PATHS     = ["/wp-admin", "/cart", "/checkout"]
USER_AGENT        = "SchemaAuditBot/1.0 (+https://github.com/)"

OUT_PREFIX = os.path.join(DRIVE_OUT, "schema_audit")  # save straight to Drive
print(f"Crawling domain: {DOMAIN}")


# ============================================
# STEP 4: Utilities (crawling + schema extract)
# ============================================
import re, asyncio, urllib.parse, io, json
from urllib.parse import urljoin, urlparse, urlunparse
from collections import defaultdict
import tldextract, pandas as pd
from tqdm.auto import tqdm
import aiohttp
from aiolimiter import AsyncLimiter
from w3lib.html import get_base_url
from selectolax.parser import HTMLParser
import lxml.etree as ET
import extruct

def normalize_url(url, base=None, strip_query=True):
    if base:
        url = urljoin(base, url)
    p = urlparse(url)
    if strip_query:
        p = p._replace(query="", fragment="")
    return urlunparse((p.scheme, p.netloc, p.path or "/", p.params, p.query, ""))

def is_same_site(url, root):
    u, r = urlparse(url), urlparse(root)
    tu, tr = tldextract.extract(u.netloc), tldextract.extract(r.netloc)
    return (tu.domain, tu.suffix) == (tr.domain, tr.suffix)

def allowed_path(path, include_list, exclude_list):
    if include_list and not any(path.startswith(p) for p in include_list):
        return False
    if any(path.startswith(p) for p in exclude_list):
        return False
    return True

async def fetch_text(session, url, timeout, render=False):
    async with session.get(url, timeout=timeout) as r:
        r.raise_for_status()
        return await r.text()

def parse_sitemaps(xml_str):
    urls = set()
    try:
        root = ET.fromstring(xml_str.encode("utf-8"))
    except Exception:
        return urls
    ns = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}
    for loc in root.findall(".//sm:url/sm:loc", ns):
        urls.add(loc.text.strip())
    return urls

def extract_structured_data(html, url):
    try:
        data = extruct.extract(
            html,
            base_url=url,
            syntaxes=["json-ld","microdata","rdfa"],
            uniform=True
        )
        return data, None
    except Exception as e:
        return None, str(e)

async def extract_links(html, base_url):
    links = set()
    try:
        doc = HTMLParser(html)
        for a in doc.css("a[href]"):
            href = a.attributes.get("href")
            if not href: continue
            url = normalize_url(href, base=base_url, strip_query=not INCLUDE_QUERYSTR)
            links.add(url)
    except: pass
    return links

async def crawl(root, mode="sitemap", max_pages=100):
    results, errors, seen, queue = [], [], set(), [root]
    limiter = AsyncLimiter(REQUESTS_PER_SEC, time_period=1)
    timeout = aiohttp.ClientTimeout(total=TIMEOUT_SECS)
    headers = {"User-Agent": USER_AGENT}

    async with aiohttp.ClientSession(headers=headers, timeout=timeout) as session:
        pbar = tqdm(total=max_pages, desc="Crawling")
        while queue and len(seen) < max_pages:
            url = queue.pop(0)
            if url in seen or not is_same_site(url, root): continue
            if not allowed_path(urlparse(url).path, ALLOWED_PATHS, EXCLUDE_PATHS):
                continue
            seen.add(url)
            try:
                async with limiter:
                    html = await fetch_text(session, url, TIMEOUT_SECS)
                data, err = extract_structured_data(html, url)
                sd_items, schema_types = [], []
                if data:
                    for syntax in ("json-ld","microdata","rdfa"):
                        for item in data.get(syntax, []):
                            sd_items.append({"syntax": syntax, "item": item})
                            t = item.get("@type") or item.get("type")
                            if isinstance(t, list): schema_types.extend(t)
                            elif isinstance(t, str): schema_types.append(t)
                results.append({
                    "url": url,
                    "has_schema": bool(sd_items),
                    "schema_types": list(set(map(str, schema_types))),
                    "num_items": len(sd_items),
                    "raw": data or {}
                })
                if mode in ("crawl","both"):
                    new_links = await extract_links(html, url)
                    queue.extend(new_links - seen)
                pbar.update(1)
            except Exception as e:
                errors.append({"url": url, "error": str(e)})
        pbar.close()
    return results, errors

# ============================================
# STEP 5: Run the crawl & Save results
# ============================================
results, errors = await crawl(DOMAIN, mode=PRIORITY, max_pages=MAX_PAGES)

df = pd.DataFrame(results)
if df.empty:
    print("No pages crawled.")
else:
    total = len(df)
    with_sd = int(df["has_schema"].sum())
    coverage = (with_sd / total * 100)
    print(f"Total: {total}, With schema: {with_sd}, Coverage: {coverage:.2f}%")

    df.to_csv(f"{OUT_PREFIX}_raw.csv", index=False)
    df[df["has_schema"]].to_csv(f"{OUT_PREFIX}_pages_with_schema.csv", index=False)
    df[~df["has_schema"]].to_csv(f"{OUT_PREFIX}_pages_without_schema.csv", index=False)

    # per-type summary
    df_types = df.explode("schema_types")
    if "schema_types" in df_types and df_types["schema_types"].notna().any():
        per_type = (
            df_types.groupby("schema_types")["url"]
            .nunique().sort_values(ascending=False).rename("pages_with_type")
        )
        per_type.to_csv(f"{OUT_PREFIX}_per_type.csv")
        display(per_type.head(20))

    pd.DataFrame(errors).to_csv(f"{OUT_PREFIX}_errors.csv", index=False)

print("\nSaved to Drive:", DRIVE_OUT)
